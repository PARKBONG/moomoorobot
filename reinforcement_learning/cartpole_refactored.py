# conda activate cobot_py311
# pip install gymnasium torch matplotlib

import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import os
import glob
import torch.nn.functional as F
import threading
import time
from pathlib import Path
from typing import Tuple, Optional

# ============================================================================
# 1. DQN Network
# ============================================================================
class DQN(nn.Module):
    def __init__(self, state_dim: int, action_dim: int):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)


# ============================================================================
# 2. Replay Memory
# ============================================================================
class ReplayMemory:
    def __init__(self, capacity: int):
        self.memory = deque(maxlen=capacity)

    def push(self, transition):
        self.memory.append(transition)

    def sample(self, batch_size: int):
        return random.sample(self.memory, batch_size)

    def __len__(self) -> int:
        return len(self.memory)


# ============================================================================
# 3. Environment Wrapper
# ============================================================================
class CartPoleEnv:
    def __init__(self):
        self.env = gym.make("CartPole-v1")
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.n

    def reset(self) -> torch.Tensor:
        state, _ = self.env.reset()
        return torch.tensor(state, dtype=torch.float32), state

    def step(self, action: int) -> Tuple[torch.Tensor, float, bool, np.ndarray]:
        next_state, reward, done, _, _ = self.env.step(action)
        next_state_tensor = torch.tensor(next_state, dtype=torch.float32)
        return next_state_tensor, reward, done, next_state

    def close(self):
        self.env.close()


# ============================================================================
# 4. DQN Trainer (Headless - í•™ìŠµë§Œ ë‹´ë‹¹)
# ============================================================================
class DQNTrainer:
    def __init__(self, env: CartPoleEnv, model_save_dir: str, 
                 gamma: float = 0.99, lr: float = 0.0005,
                 batch_size: int = 100, memory_size: int = 5000):
        self.env = env
        self.model_save_dir = Path(model_save_dir)
        self.model_save_dir.mkdir(exist_ok=True)

        self.gamma = gamma
        self.lr = lr
        self.batch_size = batch_size

        # ë„¤íŠ¸ì›Œí¬
        self.policy_net = DQN(env.state_dim, env.action_dim)
        self.target_net = DQN(env.state_dim, env.action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        # ìµœì í™”ê¸°
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)

        # ë©”ëª¨ë¦¬
        self.memory = ReplayMemory(memory_size)

        # í†µê³„
        self.episode_rewards = []
        self.step_count = 0

    def select_action(self, state: torch.Tensor) -> int:
        """Softmax ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ"""
        with torch.no_grad():
            q_value = self.target_net(state)
            p = F.softmax(q_value, dim=0).numpy()
            p /= p.sum()
            action = np.random.choice(self.env.action_dim, p=p)
        return int(action)

    def optimize_model(self):
        """DQN ì†ì‹¤ í•¨ìˆ˜ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        if len(self.memory) < self.batch_size:
            return

        transitions = self.memory.sample(self.batch_size)
        batch = list(zip(*transitions))

        state_batch = torch.stack(batch[0])
        action_batch = torch.tensor(batch[1]).unsqueeze(1)
        reward_batch = torch.tensor(batch[2])
        next_state_batch = torch.stack(batch[3])
        done_batch = torch.tensor(batch[4], dtype=torch.float32)

        q_values = self.policy_net(state_batch).gather(1, action_batch)
        next_q_values = self.target_net(next_state_batch).max(1)[0].detach()
        target_q_values = reward_batch + (self.gamma * next_q_values * (1 - done_batch))
        
        loss = nn.MSELoss()(q_values.squeeze(), target_q_values)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def train_episode(self) -> float:
        """í•œ ì—í”¼ì†Œë“œ í•™ìŠµ"""
        state, _ = self.env.reset()
        total_reward = 0

        while total_reward < 501:
            action = self.select_action(state)
            next_state, reward, done, next_state_np = self.env.step(action)

            self.memory.push((state, action, reward, next_state, done))
            state = next_state
            total_reward += reward
            self.step_count += 1

            self.optimize_model()

            if done:
                break

        # ì£¼ê¸°ì ìœ¼ë¡œ target_net ì—…ë°ì´íŠ¸
        if len(self.episode_rewards) % 20 == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        return total_reward

    def save_model(self, episode: int):
        """ëª¨ë¸ ì €ì¥"""
        model_path = self.model_save_dir / f"dqn_episode_{episode:05d}_reward_{self.episode_rewards[-1]:.0f}.pth"
        torch.save(self.policy_net.state_dict(), model_path)

    def train(self, num_episodes: int, save_interval: int = 100):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        print(f"ğŸš€ Training started - {num_episodes} episodes")
        print(f"ğŸ“ Model save directory: {self.model_save_dir}")

        for episode in range(num_episodes):
            reward = self.train_episode()
            self.episode_rewards.append(reward)

            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                print(f"[Episode {episode + 1:4d}] Avg Reward (100): {avg_reward:6.2f} | Total steps: {self.step_count}")

            if (episode + 1) % save_interval == 0:
                self.save_model(episode + 1)

        print(f"\nâœ… Training completed!")
        print(f"Final avg reward: {np.mean(self.episode_rewards[-100:]):.2f}")
        print(f"Total steps: {self.step_count}")


# ============================================================================
# 5. CartPole Visualizer (matplotlib)
# ============================================================================
def draw_cartpole(ax, state: np.ndarray, title: str = "CartPole State"):
    """matplotlibìœ¼ë¡œ CartPole ìƒíƒœ ê·¸ë¦¬ê¸°"""
    cart_x = state[0]
    pole_angle = state[2]

    # ì¹´íŠ¸ ê·¸ë¦¬ê¸°
    cart_width = 0.3
    cart_height = 0.15
    cart = plt.Rectangle((cart_x - cart_width / 2, -cart_height / 2),
                         cart_width, cart_height, color='blue', fill=True)
    ax.add_patch(cart)

    # í´(ë§‰ëŒ€) ê·¸ë¦¬ê¸°
    pole_length = 0.5
    pole_x_end = cart_x + pole_length * np.sin(pole_angle)
    pole_y_end = pole_length * np.cos(pole_angle)

    ax.plot([cart_x, pole_x_end], [0, pole_y_end], 'r-', linewidth=3)
    ax.plot([pole_x_end], [pole_y_end], 'ro', markersize=8)

    ax.set_xlim(-2.5, 2.5)
    ax.set_ylim(-0.5, 1)
    ax.set_aspect('equal')
    ax.grid(True, alpha=0.3)
    ax.set_xlabel('Position')
    ax.set_ylabel('Height')
    ax.set_title(title, fontsize=11)

    info_text = f"Pos: {cart_x:.2f}\nAngle: {np.degrees(pole_angle):.1f}Â°"
    ax.text(-2.3, 0.8, info_text, fontsize=9,
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))


# ============================================================================
# 6. Inference + Visualizer (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ)
# ============================================================================
class DQNVisualizer(threading.Thread):
    def __init__(self, model_dir: str, state_dim: int, action_dim: int):
        super().__init__(daemon=True)
        self.model_dir = Path(model_dir)
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.running = True

        # ë„¤íŠ¸ì›Œí¬
        self.net = DQN(state_dim, action_dim)
        self.net.eval()

        # í™˜ê²½ (headless)
        self.env = gym.make("CartPole-v1")

        # í†µê³„
        self.episode_count = 0
        self.latest_reward = 0
        self.latest_state = None
        self.latest_state_history = []  # ìƒíƒœ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.replay_progress = 0  # ë¦¬í”Œë ˆì´ ì§„í–‰ë„ (0.0 ~ 1.0)

        # ì‹œê°í™” ë°ì´í„° (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì ‘ê·¼)
        self.episode_rewards = []
        self.last_model_path = None
        self.new_data_available = False

    def load_latest_model(self) -> bool:
        """ìµœì‹  ëª¨ë¸ íŒŒì¼ ë¡œë“œ"""
        model_files = sorted(self.model_dir.glob("*.pth"))
        if not model_files:
            return False

        latest_model = model_files[-1]

        # ì´ë¯¸ ë¡œë“œí•œ ëª¨ë¸ì´ë©´ ìŠ¤í‚µ
        if latest_model == self.last_model_path:
            return False

        try:
            self.net.load_state_dict(torch.load(latest_model, weights_only=True))
            self.last_model_path = latest_model
            
            # íŒŒì¼ëª…ì—ì„œ ì—í”¼ì†Œë“œ ìˆ˜ì™€ ë¦¬ì›Œë“œ ì¶”ì¶œ
            filename = latest_model.stem
            if "reward" in filename:
                reward_str = filename.split("reward_")[-1]
                self.latest_reward = float(reward_str)
            
            return True
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            return False

    def inference_episode(self) -> Tuple[float, list]:
        """ì¶”ë¡  ì—í”¼ì†Œë“œ ì‹¤í–‰ (ìƒíƒœ íˆìŠ¤í† ë¦¬ ì €ì¥)"""
        state, _ = self.env.reset()
        total_reward = 0
        state_history = [state.copy()]  # ì´ˆê¸° ìƒíƒœ ì €ì¥

        while total_reward < 501:
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float32)
                q_values = self.net(state_tensor)
                action = q_values.argmax().item()

            next_state, reward, done, _, _ = self.env.step(action)
            state = next_state
            total_reward += reward
            state_history.append(state.copy())  # ëª¨ë“  ìƒíƒœ ì €ì¥
            self.latest_state = state

            if done:
                break

        return total_reward, state_history

    def run(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì¶”ë¡  ë£¨í”„ (matplotlib ì œì™¸)"""
        print("\nğŸ¨ Inference thread started - waiting for models...")

        while self.running:
            # ìµœì‹  ëª¨ë¸ ë¡œë“œ ì‹œë„
            if self.load_latest_model():
                # ì¶”ë¡  ì‹¤í–‰ (ìƒíƒœ íˆìŠ¤í† ë¦¬ í¬í•¨)
                reward, state_history = self.inference_episode()
                self.episode_rewards.append(reward)
                self.latest_state_history = state_history
                self.replay_progress = 0
                self.episode_count += 1

                print(f"[Inference {self.episode_count}] Reward: {reward:.0f} (Steps: {len(state_history)})")
                self.new_data_available = True

            time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì²´í¬

    def stop(self):
        """ìŠ¤ë ˆë“œ ì¢…ë£Œ"""
        self.running = False
        self.env.close()


# ============================================================================
# 7. Visualization Updater (ë³„ë„ ìŠ¤ë ˆë“œ)
# ============================================================================
class VisualizationUpdater(threading.Thread):
    def __init__(self, ax1, ax2, ax3, trainer: DQNTrainer, visualizer: DQNVisualizer):
        super().__init__(daemon=True)
        self.ax1 = ax1
        self.ax2 = ax2
        self.ax3 = ax3
        self.trainer = trainer
        self.visualizer = visualizer
        self.running = True
        
        self.replay_frame_idx = 0
        self.last_episode_count = 0

    def run(self):
        """ì‹œê°í™” ì—…ë°ì´íŠ¸ ë£¨í”„ (ë³„ë„ ìŠ¤ë ˆë“œ)"""
        print("ğŸ¨ Visualization updater thread started...")
        
        while self.running:
            try:
                # CartPole í”„ë ˆì„ ì§„í–‰
                if self.visualizer.latest_state_history:
                    if self.visualizer.episode_count != self.last_episode_count:
                        self.replay_frame_idx = 0
                        self.last_episode_count = self.visualizer.episode_count
                        print(f"âœ¨ New replay loaded! (Episode {self.visualizer.episode_count})")
                    
                    if self.replay_frame_idx >= len(self.visualizer.latest_state_history):
                        self.replay_frame_idx = 0
                    
                    self.replay_frame_idx += 1
                
                # matplotlib ì—…ë°ì´íŠ¸ (ìŠ¤ë ˆë“œì—ì„œ)
                self.ax1.clear()
                if self.trainer.episode_rewards:
                    self.ax1.plot(self.trainer.episode_rewards, linewidth=2, color='blue', alpha=0.7)
                    self.ax1.set_xlabel('Training Episode')
                    self.ax1.set_ylabel('Total Reward')
                    self.ax1.set_title(f'Training Progress (Episode {len(self.trainer.episode_rewards)})')
                    self.ax1.grid(True, alpha=0.3)

                self.ax2.clear()
                if len(self.trainer.episode_rewards) >= 50:
                    window = 50
                    moving_avg = np.convolve(self.trainer.episode_rewards, np.ones(window) / window, mode='valid')
                    self.ax2.plot(moving_avg, linewidth=2, color='red')
                    self.ax2.set_xlabel('Training Episode')
                    self.ax2.set_ylabel(f'{window}-Episode Avg Reward')
                    self.ax2.set_title(f'Training Moving Average: {moving_avg[-1]:.2f}')
                    self.ax2.grid(True, alpha=0.3)

                self.ax3.clear()
                if self.visualizer.latest_state_history:
                    current_idx = min(self.replay_frame_idx - 1, len(self.visualizer.latest_state_history) - 1)
                    current_state = self.visualizer.latest_state_history[current_idx]
                    progress_percent = (current_idx / len(self.visualizer.latest_state_history)) * 100
                    
                    draw_cartpole(self.ax3, current_state,
                                f'CartPole Replay (#{self.visualizer.episode_count})\n'
                                f'Frame: {current_idx}/{len(self.visualizer.latest_state_history)-1} ({progress_percent:.0f}%)\n'
                                f'Reward: {self.visualizer.latest_reward:.0f}')
                else:
                    self.ax3.text(0.5, 0.5, 'Waiting for inference...',
                                ha='center', va='center', fontsize=14)
                    self.ax3.set_xlim(-3, 3)
                    self.ax3.set_ylim(-1, 1)

                plt.tight_layout()
                plt.pause(0.05)  # 50ms ì—…ë°ì´íŠ¸

            except Exception as e:
                print(f"âš ï¸ Visualization update error: {e}")
                time.sleep(0.1)

    def stop(self):
        self.running = False



# ============================================================================
# 8. Main
# ============================================================================
def main():
    # ì„¤ì •
    NUM_EPISODES = 5000
    SAVE_INTERVAL = 100
    MODEL_DIR = "dqn_saved_models"

    # í™˜ê²½ ìƒì„±
    env = CartPoleEnv()
    print(f"ğŸ“Š Environment: CartPole-v1")
    print(f"   State Dim: {env.state_dim}")
    print(f"   Action Dim: {env.action_dim}")

    # í•™ìŠµê¸° ìƒì„±
    trainer = DQNTrainer(env, MODEL_DIR)

    # ì‹œê°í™” ìŠ¤ë ˆë“œ ì‹œì‘ (ì¶”ë¡  ë‹´ë‹¹)
    visualizer = DQNVisualizer(MODEL_DIR, env.state_dim, env.action_dim)
    visualizer.start()

    try:
        print(f"\nğŸš€ Starting training and visualization...")
        print(f"ğŸ’¡ Training runs in main thread")
        print(f"ğŸ¨ Visualization updates in separate thread")
        
        # matplotlib ìœˆë„ìš° ìƒì„± (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ)
        plt.ion()
        fig = plt.figure(figsize=(16, 5))
        ax1 = plt.subplot(131)
        ax2 = plt.subplot(132)
        ax3 = plt.subplot(133)
        
        # ì‹œê°í™” ì—…ë°ì´í„° ìŠ¤ë ˆë“œ ì‹œì‘
        vis_updater = VisualizationUpdater(ax1, ax2, ax3, trainer, visualizer)
        vis_updater.start()
        
        # í•™ìŠµ ë£¨í”„ (ë©”ì¸ ìŠ¤ë ˆë“œ)
        print(f"[Training] Starting {NUM_EPISODES} episodes...")
        for episode in range(NUM_EPISODES):
            reward = trainer.train_episode()
            trainer.episode_rewards.append(reward)

            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(trainer.episode_rewards[-100:])
                print(f"[Episode {episode + 1:4d}] Avg Reward (100): {avg_reward:6.2f} | Total steps: {trainer.step_count}")

            if (episode + 1) % SAVE_INTERVAL == 0:
                trainer.save_model(episode + 1)

        print(f"\nâœ… Training completed!")
        print(f"Final avg reward: {np.mean(trainer.episode_rewards[-100:]):.2f}")
        
        # ì‹œê°í™” ê³„ì† í‘œì‹œ
        print("ğŸ“Š Visualization will continue updating. Close the window to exit.")
        vis_updater.running = True  # ê³„ì† ì‹¤í–‰
        plt.show()  # ë¸”ë¡œí‚¹

    except KeyboardInterrupt:
        print("\nâš ï¸ Interrupted by user")
    finally:
        print("ğŸ›‘ Stopping all threads...")
        vis_updater.stop()
        visualizer.stop()
        visualizer.join(timeout=2)
        env.close()
        plt.close('all')
        print("âœ… All completed!")


if __name__ == "__main__":
    main()
